# -*- coding: utf-8 -*-
"""code python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-7SLljVoGVzMtDZYTZHDbXwCI4wg92wt
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from tensorflow import keras
import random 
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten, Dropout,BatchNormalization
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.utils import plot_model
from tensorflow.keras.datasets import mnist,cifar10, cifar100
# %load_ext tensorboard
from tensorflow.keras.callbacks import EarlyStopping
import datetime, os
from tensorflow.keras.models import load_model
from keras.preprocessing.image import ImageDataGenerator
import scipy.io
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

def load_minist():

     (trainX, trainY), (testX, testY) = mnist.load_data()
     
     trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
     testX = testX.reshape((testX.shape[0], 28, 28, 1))
     trainY = to_categorical(trainY)
     testY = to_categorical(testY)
      
     train_norm =trainX.astype('float32')
     test_norm = testX.astype('float32')
    
     train_norm = train_norm / 255.0
     test_norm = test_norm / 255.0
     
     return train_norm, trainY,test_norm, testY

trainX, trainY, testX, testY=load_minist()

def load_cifar10():
    
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)

    train_norm =trainX.astype('float32')
    test_norm = testX.astype('float32')
    
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    
    return train_norm, trainY,test_norm, testY

trainX, trainY, testX, testY=load_cifar10()

def load_cifar100():
    
    (trainX, trainY), (testX, testY) = cifar100.load_data()
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)

    train_norm =trainX.astype('float32')
    test_norm = testX.astype('float32')
    
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    
    return train_norm, trainY,test_norm, testY

trainX, trainY, testX, testY=load_cifar100()

def load_dataset():

      mat = scipy.io.loadmat('rede_normalizationrelu50001.mat')

      train=mat['train']
      trainy=mat['trainy']

      val=mat['val']
      valy=mat['valy']

      train=train.T
      trainy=trainy.T

      val=val.T
      valy=valy.T

      scaler=MinMaxScaler()

      scaler.fit(train)

      valmod = scaler.transform(val)
      train = scaler.transform(train)

      scaler=MinMaxScaler()
      scaler.fit(trainy)

      trainY = scaler.transform(trainy)
      testY= scaler.transform(valy)

      x_train = np.reshape(train, (len(train), 50, 50, 1))
      x_test = np.reshape(valmod, (len(valmod), 50, 50, 1))

      return x_train, trainY, x_test, testY

trainX, trainY, testX, testY=load_dataset()

model = load_model('best_model_classification.h5') # for classfication task
model.summary()

from tensorflow.keras import backend
def rmse(y_true, y_pred):
	return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))

#model = load_model('model_original.h5', custom_objects={'rmse':rmse},compile=True) # for regression task
#model.summary()

max_conv_layers=3
max_dense_layers=3
num_para=1
convolutional_layer_shape = [
            "active",
            "active1", 
            "num_filters",
            "activacao",
            "init",
            "dropout",
            "pool"
           
        ]
   # pooling_layer=["pool"]
dense_layer_shape = [
            "active",
            "num_densa",
            "activacao",
            "init",
            "dropout"
        ]
parametro=['batch_size']

best_model_permeability=[1, 1, 256, 'elu', 'glorot_uniform', 0.2, 'AveragePooling2D', 1, 1, 32, 'elu', 'normal', 0.3, 'AveragePooling2D', 0, 0, 64, 'elu', 'glorot_uniform', 0.3, 'MaxPooling2D', 0, 16, 'relu', 'glorot_uniform', 0.2, 0, 128, 'relu', 'glorot_normal', 0.2, 0, 128, 'elu', 'glorot_normal', 0.2, 'glorot_normal', 10, 'rmsprop'] #  chomossome

cross=best_model_permeability

classes= 10 #  number of classes of the problem.

input_shape=(trainX.shape[1],trainX.shape[2],trainX.shape[3])
#input_shape=(50,50,1)

convolution_layer_size = len(convolutional_layer_shape)
dense_layer_size=len(dense_layer_shape)
offset = 0
input_layer = True
model = Sequential()
for i in range(max_conv_layers):
            if cross[offset]:
                convolution = None
                if input_layer:
                    convolution = Conv2D(
                        cross[offset + 2], (3, 3), activation=cross[offset + 3], kernel_initializer=cross[offset + 4],
                        padding='same',
                        input_shape=input_shape
                    )
                    input_layer = False
                else:
                    convolution = Conv2D(
                        cross[offset + 2], (3, 3),activation=cross[offset + 3] , kernel_initializer=cross[offset + 4],
                        padding='same' 
                    )                   
                model.add(convolution)
                model.add(BatchNormalization())
                    
                if (cross[offset + 1]==1):    
                     convolution = Conv2D(cross[offset + 2], (3, 3),activation=cross[offset + 3], kernel_initializer=cross[offset + 4], padding='same' )
                     model.add(convolution)
                     model.add(BatchNormalization())
                    
                if (cross[offset+6]=='MaxPooling2D'):  
                        model.add(MaxPooling2D((2, 2)))
                        model.add(Dropout(cross[offset + 5]))

                else:  
                        model.add(AveragePooling2D((2, 2)))
                        model.add(Dropout(cross[offset + 5]))
                
            offset+=convolution_layer_size
             
if not input_layer:
  model.add(Flatten())
for i in range(max_dense_layers):
        if cross[offset]:
                dense = Dense(cross[offset + 1],activation=cross[offset + 2],  kernel_initializer=cross[offset + 3])
                model.add(dense)
                model.add(BatchNormalization())
                model.add(Dropout(cross[offset + 4]))
                
        offset +=dense_layer_size

model.add(Dense(classes, activation='softmax', kernel_initializer=cross[offset])) # activation function "softmax" for the classification task, and "linear" for the res regression task
model.compile(optimizer=cross[len(cross)-1], loss='categorical_crossentropy', metrics=['accuracy'])   #activation function "accuracy" for the classification task, and "rmse" for the res regression task

geral=model.fit(trainX, trainY, epochs=400, batch_size= cross[len(cross)-2], validation_data=(testX, testY), verbose=2)